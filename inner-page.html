<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Trash-Pot</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/logo.png" rel="icon">
  <link href="assets/img/logo.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Selecao - v4.9.1
  * Template URL: https://bootstrapmade.com/selecao-bootstrap-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center header-scrolled">
    <div class="container d-flex align-items-center justify-content-between">

      <div class="logo">
        <!-- <h1><a href="index.html">ECO Agri Solutions</a></h1> -->
        <!-- Uncomment below if you prefer to use an image logo -->
        <a href="index.html"><img src="./ECO Agri Solutions-Domain_files/ECO logo transparent 03.png" alt="" class="img-fluid"></a>
      </div>

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" style="color: aliceblue;" href="index.html">Home</a></li>
       </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <!-- <h2>Domain</h2> -->
          <ol>
            <li><a href= "index.html">Home</a></li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <section class="inner-page">
      <div class="container" style="text-align: justify;">
        <b><h2 style="text-align: center;">Literature Survey</h2></b><br>
        <p>
          Federated learning is a cutting-edge machine learning paradigm that addresses the critical challenges of privacy, data decentralization, and collaborative model training. This innovative approach enables the development of robust machine learning models without the need to centralize sensitive or distributed data, making it particularly valuable in today's data-driven world.
          The process of federated learning is collaborative and iterative. It begins with the creation of a global model hosted on a central server or in the cloud. This global model is then shared with the participating devices or servers. Each of these local entities independently computes model updates using its own data while preserving data privacy. These updates, often represented as model parameters, are then transmitted back to the central server for aggregation. Through this process, the global model is continually refined and improved .
          Privacy is a cornerstone of federated learning. It ensures that personal or sensitive data never leaves the device or data source, greatly reducing the risk of data breaches and privacy violations. Differential privacy techniques are often integrated into federated learning to add noise or randomness to the model updates, further safeguarding individual data .        </p>
        <h4>01. Data poisoning attack.</h4>
		<p>Data poisoning attacks in federated learning are malicious attempts to compromise the integrity and effectiveness of machine learning models trained in a decentralized, collaborative environment. These attacks exploit the collaborative nature of federated learning, where multiple devices or data sources contribute to model training.
      In a data poisoning attack, a malicious participant intentionally injects manipulated or poisoned data into the federated learning system. This poisoned data is designed to deceive the learning algorithm and compromise the final model's performance. The attacker's goal may vary, from causing the model to make incorrect predictions to revealing sensitive information embedded in the model's parameters.
</p>
          <h4>02. Byzantine attack .</h4>
		  <p>
        Byzantine attacks in federated learning are a class of malicious activities where some participants in a decentralized network behave dishonestly or arbitrarily to compromise the integrity of the federated learning process. These participants, known as Byzantine or malicious nodes, deviate from the prescribed protocol by intentionally providing incorrect or misleading information during model aggregation.
        Byzantine attacks can severely impact the accuracy, security, and reliability of federated learning. Ensuring the security and trustworthiness of federated learning systems is essential to protect against Byzantine attacks and maintain the effectiveness of collaborative model training.
</P>
      
      <h4>03. Model poisoning attack  </h4>
      <p>Model poisoning attacks in federated learning are a form of malicious activity where an attacker aims to compromise the global model's integrity and performance by injecting strategically crafted model updates from a compromised participant. Unlike data
        poisoning attacks, which manipulate training data, model poisoning attacks focus on manipulating the model parameters themselves.
        Model poisoning attacks can have serious consequences, such as compromising the security of the federated learning system, introducing backdoors into the model, or undermining the model's intended functionality. Defending against model poisoning attacks requires robust mechanisms for detecting and mitigating malicious updates, as well as ensuring the integrity and trustworthiness of the participants in the federated learning process.
</p>

        <h4>04.Sybil attack.</h4>
        <p> 
					Sybil attacks in federated learning are a type of malicious activity where an adversary creates multiple fake or unauthorized participant identities within the decentralized network, with the intent to manipulate the federated learning process. These fake identities, known as Sybil nodes, are controlled by a single attacker, and are used to influence the model's training or disrupt the federated learning process.
Sybil attacks pose a significant threat to the security and fairness of federated learning systems. Preventing Sybil attacks is essential to maintain the trustworthiness and
effectiveness of federated learning systems, especially in scenarios where data privacy and integrity are paramount.

          </p>




          <b><h2 style="text-align: center;">Research Problem </h2></b><br>
        <p>
          Federated learning, despite its promise in preserving data privacy and enabling decentralized machine learning, is susceptible to various vulnerabilities that can impact its security and effectiveness. Some of the key risks and challenges associated with federated learning are poisoning attacks, Byzantine attacks, and Sybil attacks.
          The probability of attacks occurring within a federated learning environment is a complex and multifaceted concern that arises from the intersection of machine learning, privacy, and cybersecurity. Attacks on federated learning can have significant and wide-ranging consequences that impact the security, privacy, and functionality of the system. These consequences can vary in severity depending on the nature of the attack and the effectiveness of countermeasures in place. One of the primary consequences of attacks is the degradation of the federated machine learning model's quality and performance. Adversarial interference can introduce bias, reduce accuracy, or even render the model unusable. This can undermine the intended benefits of federated learning, such as improved model generalization. Attacks may compromise the privacy of individual data sources within the federated network. If adversaries gain access to sensitive information through model updates or other means, it can lead to privacy violations and data leaks,
          eroding trust among participants. Some attacks may consume excessive computational resources, causing system slowdowns or failures. Resource-constrained devices, such as IoT sensors, may be particularly vulnerable to these resource-draining attacks, impacting their ability to participate in federated learning. Successful attacks can erode trust among participants in the federated learning ecosystem. If entities cannot rely on the integrity and security of the collaborative process, they may be reluctant to participate, hindering the effectiveness of federated learning initiatives.
          Detecting attacks in federated learning is of paramount importance due to the critical role it plays in safeguarding the integrity, security, and privacy of the federated learning ecosystem. It not only helps protect sensitive data and maintain model integrity but also fosters trust among participants and ensures the continued viability of federated learning in applications where data privacy and security are paramount. Therefore, the main intention of this research is to implement methods to detect attacks towards federated learning environment.        
          
		   
		   
		   
      </div>
    </section>

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <h3>FedSec Advanced Threat Detection System</h3>
  
     
      <div class="copyright">
        © Copyright <strong><span>FedSec Advanced Threat Detection System</span></strong>. All Rights Reserved
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>